{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db1454f5-1c99-4e17-82e4-01bf5d60c7bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (4058890995.py, line 132)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 132\u001b[0;36m\u001b[0m\n\u001b[0;31m    parser.add_argument(\"--swa_annealing_epochs\", type=int, default=3, helpÂ |oai:code-citation|\u001b[0m\n\u001b[0m                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import hashlib\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Define custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataframe.iloc[idx]['image']\n",
    "        image = Image.open(image).convert('RGB')\n",
    "        label = self.dataframe.iloc[idx]['binaryLabel']\n",
    "        protected_attr = self.dataframe.iloc[idx]['Age_Category']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, protected_attr\n",
    "\n",
    "# Data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Arguments collection\n",
    "def collect_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # experiments\n",
    "    parser.add_argument('--experiment', type=str, choices=[\n",
    "        'baseline', 'CFair', 'LAFTR', 'LNL', 'EnD', 'DomainInd', 'resampling',\n",
    "        'ODR', 'SWA', 'SWAD', 'SAM', 'GSAM', 'SAMSWAD', 'GroupDRO', 'BayesCNN',\n",
    "        'resamplingSWAD'\n",
    "    ])\n",
    "    parser.add_argument('--experiment_name', type=str, default='test')\n",
    "    parser.add_argument('--wandb_name', type=str, default='baseline')\n",
    "    parser.add_argument('--if_wandb', type=bool, default=True)\n",
    "    parser.add_argument('--dataset_name', default='CXP', choices=[\n",
    "        'CXP', 'NIH', 'MIMIC_CXR', 'RadFusion', 'RadFusion4', 'HAM10000', \n",
    "        'HAM100004', 'Fitz17k', 'OCT', 'PAPILA', 'ADNI', 'ADNI3T', 'COVID_CT_MD',\n",
    "        'RadFusion_EHR', 'MIMIC_III', 'eICU'\n",
    "    ])\n",
    "    parser.add_argument('--resume_path', type=str, default='', help='explicitly identify checkpoint path to resume.')\n",
    "    parser.add_argument('--sensitive_name', default='Sex', choices=['Sex', 'Age', 'Race', 'skin_type', 'Insurance'])\n",
    "    parser.add_argument('--is_3d', type=bool, default=False)\n",
    "    parser.add_argument('--is_tabular', type=bool, default=False)\n",
    "\n",
    "    # training \n",
    "    parser.add_argument('--random_seed', type=int, default=0)\n",
    "    parser.add_argument('--batch_size', type=int, default=1024)\n",
    "    parser.add_argument('--no_cuda', dest='cuda', action='store_false')\n",
    "    parser.add_argument('--lr', type=float, default=1e-4, help='learning rate')\n",
    "    parser.add_argument('--weight_decay', type=float, default=1e-4, help='weight decay for optimizer')\n",
    "    parser.add_argument('--lr_decay_rate', type=float, default=0.1, help='decay rate of the learning rate')\n",
    "    parser.add_argument('--lr_decay_period', type=float, default=10, help='decay period of the learning rate')\n",
    "    parser.add_argument('--total_epochs', type=int, default=15, help='total training epochs')\n",
    "    parser.add_argument('--early_stopping', type=int, default=5, help='early stopping epochs')\n",
    "    parser.add_argument('--test_mode', type=bool, default=False, help='if using test mode')\n",
    "    parser.add_argument('--hyper_search', type=bool, default=False, help='if searching hyper-parameters')\n",
    "\n",
    "    # testing\n",
    "    parser.add_argument('--hash_id', type=str, default='')\n",
    "\n",
    "    # strategy for validation\n",
    "    parser.add_argument('--val_strategy', type=str, default='loss', choices=['loss', 'worst_auc'], help='strategy for selecting val model')\n",
    "\n",
    "    # cross-domain\n",
    "    parser.add_argument('--cross_testing', action='store_true')\n",
    "    parser.add_argument('--source_domain', default='', choices=['CXP', 'MIMIC_CXR', 'ADNI', 'ADNI3T'])\n",
    "    parser.add_argument('--target_domain', default='', choices=['CXP', 'MIMIC_CXR', 'ADNI', 'ADNI3T'])\n",
    "    parser.add_argument('--cross_testing_model_path', type=str, default='', help='path of the models of three random seeds')\n",
    "    parser.add_argument('--cross_testing_model_path_single', type=str, default='', help='path of the models')\n",
    "\n",
    "    # network\n",
    "    parser.add_argument('--backbone', default='cusResNet18', choices=[\n",
    "        'cusResNet18', 'cusResNet50', 'cusDenseNet121', 'cusResNet18_3d', \n",
    "        'cusResNet50_3d', 'cusMLP'\n",
    "    ])\n",
    "    parser.add_argument('--pretrained', type=bool, default=True, help='if use pretrained ResNet backbone')\n",
    "    parser.add_argument('--output_dim', type=int, default=14, help='output dimension of the classification network')\n",
    "    parser.add_argument('--num_classes', type=int, default=14, help='number of target classes')\n",
    "    parser.add_argument('--sens_classes', type=int, default=2, help='number of sensitive classes')\n",
    "    parser.add_argument('--input_channel', type=int, default=3, help='input channel of the images')\n",
    "\n",
    "    # resampling\n",
    "    parser.add_argument('--resample_which', type=str, default='group', choices=['class', 'balanced'], help='audit step for LAFTR')\n",
    "\n",
    "    # LAFTR\n",
    "    parser.add_argument('--aud_steps', type=int, default=1, help='audit step for LAFTR')\n",
    "    parser.add_argument('--class_coeff', type=float, default=1.0, help='coefficient for classification loss of LAFTR')\n",
    "    parser.add_argument('--fair_coeff', type=float, default=1.0, help='coefficient for fair loss of LAFTR')\n",
    "    parser.add_argument('--model_var', type=str, default='laftr-eqodd', help='model variation for LAFTR')\n",
    "    # CFair\n",
    "    parser.add_argument('--mu', type=float, default=0.1, help='coefficient for adversarial loss of CFair')\n",
    "\n",
    "    # LNL\n",
    "    parser.add_argument('--_lambda', type=float, default=0.1, help='coefficient for loss of LNL')\n",
    "\n",
    "    # EnD\n",
    "    parser.add_argument('--alpha', type=float, default=0.1, help='weighting parameters alpha for EnD method')\n",
    "    parser.add_argument('--beta', type=float, default=0.1, help='weighting parameters beta for EnD method')\n",
    "\n",
    "    # ODR\n",
    "    parser.add_argument(\"--lambda_e\", type=float, default=0.1, help=\"coefficient for loss of ODR\")\n",
    "    parser.add_argument(\"--lambda_od\", type=float, default=0.1, help=\"coefficient for loss of ODR\")\n",
    "    parser.add_argument(\"--gamma_e\", type=float, default=0.1, help=\"coefficient for loss of ODR\")\n",
    "    parser.add_argument(\"--gamma_od\", type=float, help=\"coefficient for loss of ODR\")\n",
    "    parser.add_argument(\"--step_size\", type=int, default=20, help=\"step size for adjusting coefficients for loss of ODR\")\n",
    "    # GroupDRO\n",
    "    parser.add_argument(\"--groupdro_alpha\", type=float, default=0.2, help=\"coefficient alpha for loss of GroupDRO\")\n",
    "    parser.add_argument(\"--groupdro_gamma\", type=float, default=0.1, help=\"coefficient gamma for loss of GroupDRO\")\n",
    "    # SWA\n",
    "    parser.add_argument(\"--swa_start\", type=int, default=7, help=\"starting epoch for averaging of SWA\")\n",
    "    parser.add_argument(\"--swa_lr\", type=float, default=0.0001, help=\"learning rate for averaging of SWA\")\n",
    "    parser.add_argument(\"--swa_annealing_epochs\", type=int, default=3, he\n",
    "        # SWAD\n",
    "    parser.add_argument(\"--swad_start\", type=int, default=7, help=\"starting epoch for averaging of SWAD\")\n",
    "    parser.add_argument(\"--swad_end\", type=int, default=14, help=\"ending epoch for averaging of SWAD\")\n",
    "    parser.add_argument(\"--swad_frequency\", type=int, default=1, help=\"frequency of SWAD averaging\")\n",
    "    # SAM and GSAM\n",
    "    parser.add_argument(\"--rho\", type=float, default=0.05, help=\"neighborhood size for SAM and GSAM\")\n",
    "    parser.add_argument(\"--eta\", type=float, default=0.01, help=\"learning rate scaling factor for GSAM\")\n",
    "    parser.add_argument(\"--lr_sam\", type=float, default=1e-4, help=\"learning rate for SAM optimizer\")\n",
    "    parser.add_argument(\"--weight_decay_sam\", type=float, default=1e-4, help=\"weight decay for SAM optimizer\")\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "# Function to evaluate model performance and bias\n",
    "def evaluate_bias(model, test_loader, protected_attr_name):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_protected_attrs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, protected_attrs in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_protected_attrs.extend(protected_attrs.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'label': all_labels,\n",
    "        'pred': all_preds,\n",
    "        protected_attr_name: all_protected_attrs\n",
    "    })\n",
    "\n",
    "    # Calculate accuracy and F1-Score for each protected group\n",
    "    age_group_accuracies = {}\n",
    "    age_group_f1_scores = {}\n",
    "    for group in df[protected_attr_name].unique():\n",
    "        group_df = df[df[protected_attr_name] == group]\n",
    "        group_accuracy = accuracy_score(group_df['label'], group_df['pred'])\n",
    "        group_f1 = f1_score(group_df['label'], group_df['pred'], average='weighted')\n",
    "        age_group_accuracies[group] = group_accuracy\n",
    "        age_group_f1_scores[group] = group_f1\n",
    "\n",
    "    # Calculate demographic parity and equalized odds\n",
    "    dp = {}\n",
    "    eo = {}\n",
    "    for group in df[protected_attr_name].unique():\n",
    "        group_df = df[df[protected_attr_name] == group]\n",
    "        dp[group] = group_df['pred'].mean()\n",
    "        eo[group] = group_df['pred'].std()\n",
    "\n",
    "    return accuracy, f1, age_group_accuracies, age_group_f1_scores, dp, eo\n",
    "\n",
    "# Function to print fairness metrics\n",
    "def print_fairness_metrics(model_name, accuracy, f1, age_group_accuracies, age_group_f1_scores, dp, eo):\n",
    "    print(f\"\\n=== Fairness Metrics for {model_name} ===\")\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Overall F1-Score: {f1:.4f}\")\n",
    "\n",
    "    print(\"\\nAccuracy by Age Group:\")\n",
    "    for age_group, group_accuracy in age_group_accuracies.items():\n",
    "        print(f\"  Age group {age_group}: {group_accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\nF1-Score by Age Group:\")\n",
    "    for age_group, group_f1 in age_group_f1_scores.items():\n",
    "        print(f\"  Age group {age_group}: {group_f1:.4f}\")\n",
    "\n",
    "    print(\"\\nDemographic Parity:\")\n",
    "    dp_df = pd.DataFrame.from_dict(dp, orient='index', columns=['Demographic Parity'])\n",
    "    print(dp_df)\n",
    "\n",
    "    print(\"\\nEqualized Odds:\")\n",
    "    eo_df = pd.DataFrame.from_dict(eo, orient='index')\n",
    "    print(eo_df)\n",
    "\n",
    "# Main function to train and evaluate models\n",
    "def main():\n",
    "    args = collect_args()\n",
    "\n",
    "    # Load and preprocess the data\n",
    "    train_df = pd.read_csv('path_to_train_data.csv')\n",
    "    test_df = pd.read_csv('path_to_test_data.csv')\n",
    "\n",
    "    train_dataset = CustomDataset(train_df, transform=transform)\n",
    "    test_dataset = CustomDataset(test_df, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # Define models (example with ResNet18)\n",
    "    if args.backbone == 'cusResNet18':\n",
    "        model = models.resnet18(pretrained=args.pretrained)\n",
    "        model.fc = nn.Linear(model.fc.in_features, args.num_classes)\n",
    "    elif args.backbone == 'cusResNet50':\n",
    "        model = models.resnet50(pretrained=args.pretrained)\n",
    "        model.fc = nn.Linear(model.fc.in_features, args.num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported backbone: {args.backbone}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    # Training loop\n",
    "    best_accuracy = 0\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(args.total_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels, _ in train_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch [{epoch + 1}/{args.total_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        accuracy, _, _, _, _, _ = evaluate_bias(model, test_loader, protected_attr_name=args.sensitive_name)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            early_stopping_counter = 0\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= args.early_stopping:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    # Load the best model for evaluation\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    accuracy, f1, age_group_accuracies, age_group_f1_scores, dp, eo = evaluate_bias(model, test_loader, protected_attr_name=args.sensitive_name)\n",
    "    print_fairness_metrics(args.backbone, accuracy, f1, age_group_accuracies, age_group_f1_scores, dp, eo)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c754d6-3835-47f9-89fa-ccc5d9de395d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
