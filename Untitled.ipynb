{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a4a30f-5402-4cff-9a5d-f75ad97be2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Additional imports for deep learning models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# read metadata\n",
    "path = 'your_path/fariness_data/HAM10000/'\n",
    "\n",
    "demo_data = pd.read_csv(path + 'HAM10000_metadata.csv')\n",
    "print(Counter(demo_data['dataset']))\n",
    "\n",
    "# add image path to the metadata\n",
    "pathlist = demo_data['image_id'].values.tolist()\n",
    "paths = ['HAM10000_images/' + i + '.jpg' for i in pathlist]\n",
    "demo_data['Path'] = paths\n",
    "\n",
    "# remove age/sex == null \n",
    "demo_data = demo_data[~demo_data['age'].isnull()]\n",
    "demo_data = demo_data[~demo_data['sex'].isnull()]\n",
    "\n",
    "# unify the value of sensitive attributes\n",
    "sex = demo_data['sex'].values\n",
    "sex[sex == 'male'] = 'M'\n",
    "sex[sex == 'female'] = 'F'\n",
    "demo_data['Sex'] = sex\n",
    "\n",
    "# split subjects to different age groups\n",
    "demo_data['Age_multi'] = demo_data['age'].values.astype('int')\n",
    "demo_data['Age_multi'] = np.where(demo_data['Age_multi'].between(-1,19), 0, demo_data['Age_multi'])\n",
    "demo_data['Age_multi'] = np.where(demo_data['Age_multi'].between(20,39), 1, demo_data['Age_multi'])\n",
    "demo_data['Age_multi'] = np.where(demo_data['Age_multi'].between(40,59), 2, demo_data['Age_multi'])\n",
    "demo_data['Age_multi'] = np.where(demo_data['Age_multi'].between(60,79), 3, demo_data['Age_multi'])\n",
    "demo_data['Age_multi'] = np.where(demo_data['Age_multi']>=80, 4, demo_data['Age_multi'])\n",
    "\n",
    "demo_data['Age_binary'] = demo_data['age'].values.astype('int')\n",
    "demo_data['Age_binary'] = np.where(demo_data['Age_binary'].between(-1, 60), 0, demo_data['Age_binary'])\n",
    "demo_data['Age_binary'] = np.where(demo_data['Age_binary']>= 60, 1, demo_data['Age_binary'])\n",
    "\n",
    "# convert to binary labels\n",
    "# benign: bcc, bkl, dermatofibroma, nv, vasc\n",
    "# malignant: akiec, mel\n",
    "\n",
    "labels = demo_data['dx'].values.copy()\n",
    "labels[labels == 'akiec'] = '1'\n",
    "labels[labels == 'mel'] = '1'\n",
    "labels[labels != '1'] = '0'\n",
    "\n",
    "labels = labels.astype('int')\n",
    "\n",
    "demo_data['binaryLabel'] = labels\n",
    "\n",
    "def split_811(all_meta, patient_ids):\n",
    "    sub_train, sub_val_test = train_test_split(patient_ids, test_size=0.2, random_state=0)\n",
    "    sub_val, sub_test = train_test_split(sub_val_test, test_size=0.5, random_state=0)\n",
    "    train_meta = all_meta[all_meta.lesion_id.isin(sub_train)]\n",
    "    val_meta = all_meta[all_meta.lesion_id.isin(sub_val)]\n",
    "    test_meta = all_meta[all_meta.lesion_id.isin(sub_test)]\n",
    "    return train_meta, val_meta, test_meta\n",
    "\n",
    "sub_train, sub_val, sub_test = split_811(demo_data, np.unique(demo_data['lesion_id']))\n",
    "\n",
    "sub_train.to_csv('your_path/fariness_data/HAM10000/split/new_train.csv')\n",
    "sub_val.to_csv('your_path/fariness_data/HAM10000/split/new_val.csv')\n",
    "sub_test.to_csv('your_path/fariness_data/HAM10000/split/new_test.csv')\n",
    "\n",
    "# you can have a look of some examples here\n",
    "img = cv2.imread('your_path/fariness_data/HAM10000/HAM10000_images/ISIC_0027419.jpg')\n",
    "print(img.shape)\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "test_meta = pd.read_csv('your_path/fariness_data/HAM10000/split/new_train.csv')\n",
    "\n",
    "path = 'your_path/fariness_data/HAM10000/pkls/'\n",
    "images = []\n",
    "start = time.time()\n",
    "for i in range(len(test_meta)):\n",
    "    img = cv2.imread(test_meta.iloc[i]['Path'])\n",
    "    # resize to the input size in advance to save time during training\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "    images.append(img)\n",
    "end = time.time()\n",
    "print(\"Time taken to load and resize images: \", end-start)\n",
    "with open(path + 'train_images.pkl', 'wb') as f:\n",
    "    pickle.dump(images, f)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['Path']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.dataframe.iloc[idx]['binaryLabel']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomDataset(dataframe=sub_train, transform=transform)\n",
    "val_dataset = CustomDataset(dataframe=sub_val, transform=transform)\n",
    "test_dataset = CustomDataset(dataframe=sub_test, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the models\n",
    "model_names = ['resnet18', 'resnet34', 'resnet50', 'vgg16', 'densenet121', 'inception_v3']\n",
    "\n",
    "models_dict = {\n",
    "    'resnet18': models.resnet18(pretrained=True),\n",
    "    'resnet34': models.resnet34(pretrained=True),\n",
    "    'resnet50': models.resnet50(pretrained=True),\n",
    "    'vgg16': models.vgg16(pretrained=True),\n",
    "    'densenet121': models.densenet121(pretrained=True),\n",
    "    'inception_v3': models.inception_v3(pretrained=True, aux_logits=False)\n",
    "}\n",
    "\n",
    "for model_name in models_dict:\n",
    "    model = models_dict[model_name]\n",
    "    if 'resnet' in model_name or 'densenet' in model_name:\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, 2)\n",
    "    elif 'vgg' in model_name:\n",
    "        num_ftrs = model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Linear(num_ftrs, 2)\n",
    "    elif 'inception' in model_name:\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, 2)\n",
    "    models_dict[model_name] = model\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        print()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the models\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trained_models = {}\n",
    "for model_name in models_dict:\n",
    "    model = models_dict[model_name].to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "    trained_model = train_model(model, dataloaders, criterion,optimizer, num_epochs=25)\n",
    "    trained_models[model_name] = trained_model\n",
    "\n",
    "# Save the models\n",
    "for model_name in trained_models:\n",
    "    torch.save(trained_models[model_name].state_dict(), f'model_{model_name}.pth')\n",
    "\n",
    "# Functions for calculating fairness metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def demographic_parity(y_true, y_pred, protected_attr):\n",
    "    groups = np.unique(protected_attr)\n",
    "    dp = {}\n",
    "    for group in groups:\n",
    "        indices = protected_attr == group\n",
    "        dp[group] = np.mean(y_pred[indices])\n",
    "    return dp\n",
    "\n",
    "def equalized_odds(y_true, y_pred, protected_attr):\n",
    "    groups = np.unique(protected_attr)\n",
    "    eo = {}\n",
    "    for group in groups:\n",
    "        indices = protected_attr == group\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true[indices], y_pred[indices]).ravel()\n",
    "        eo[group] = {'TPR': tp / (tp + fn), 'FPR': fp / (fp + tn)}\n",
    "    return eo\n",
    "\n",
    "# Evaluate bias\n",
    "def evaluate_bias(model, dataloader, protected_attr_name):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_protected_attrs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_protected_attrs.extend(dataloader.dataset.dataframe[protected_attr_name].values)\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_protected_attrs = np.array(all_protected_attrs)\n",
    "\n",
    "    dp = demographic_parity(all_labels, all_preds, all_protected_attrs)\n",
    "    eo = equalized_odds(all_labels, all_preds, all_protected_attrs)\n",
    "\n",
    "    return dp, eo\n",
    "\n",
    "# Evaluate models\n",
    "results = {}\n",
    "for model_name in trained_models:\n",
    "    dp, eo = evaluate_bias(trained_models[model_name], test_loader, protected_attr_name='Sex')\n",
    "    results[model_name] = {'Demographic Parity': dp, 'Equalized Odds': eo}\n",
    "\n",
    "for model_name in results:\n",
    "    print(f\"Results for {model_name}:\")\n",
    "    print(\"Demographic Parity: \", results[model_name]['Demographic Parity'])\n",
    "    print(\"Equalized Odds: \", results[model_name]['Equalized Odds'])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
