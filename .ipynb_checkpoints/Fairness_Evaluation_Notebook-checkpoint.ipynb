{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3489bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Additional imports for deep learning models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Read metadata\n",
    "path = '/path/to/dataset/'\n",
    "\n",
    "demo_data = pd.read_csv(path + 'HAM10000_metadata.csv')\n",
    "print(Counter(demo_data['dataset']))\n",
    "\n",
    "# Add image path to the metadata\n",
    "pathlist = demo_data['image_id'].values.tolist()\n",
    "paths = [f'HAM10000_images/{i}.png' for i in pathlist]\n",
    "demo_data['Path'] = paths\n",
    "\n",
    "# Remove rows with null values for age/sex\n",
    "demo_data = demo_data[~demo_data['age'].isnull()]\n",
    "demo_data = demo_data[~demo_data['sex'].isnull()]\n",
    "\n",
    "# Unify the value of sensitive attributes\n",
    "age_conditions = [\n",
    "    (demo_data['age'] < 20),\n",
    "    (demo_data['age'] >= 20) & (demo_data['age'] < 40),\n",
    "    (demo_data['age'] >= 40) & (demo_data['age'] < 60),\n",
    "    (demo_data['age'] >= 60) & (demo_data['age'] < 80),\n",
    "    (demo_data['age'] >= 80)\n",
    "]\n",
    "age_categories = ['child', 'young_adult', 'adult', 'senior', 'elderly']\n",
    "demo_data['Age_Category'] = np.select(age_conditions, age_categories, default='unknown')\n",
    "\n",
    "# Convert to binary labels\n",
    "labels = demo_data['dx'].values.copy()\n",
    "labels[labels == 'akiec'] = '1'\n",
    "labels[labels == 'mel'] = '1'\n",
    "labels[labels != '1'] = '0'\n",
    "labels = labels.astype('int')\n",
    "demo_data['binaryLabel'] = labels\n",
    "\n",
    "def split_811(all_meta, patient_ids):\n",
    "    sub_train, sub_val_test = train_test_split(patient_ids, test_size=0.2, random_state=0)\n",
    "    sub_val, sub_test = train_test_split(sub_val_test, test_size=0.5, random_state=0)\n",
    "    train_meta = all_meta[all_meta.lesion_id.isin(sub_train)]\n",
    "    val_meta = all_meta[all_meta.lesion_id.isin(sub_val)]\n",
    "    test_meta = all_meta[all_meta.lesion_id.isin(sub_test)]\n",
    "    return train_meta, val_meta, test_meta\n",
    "\n",
    "sub_train, sub_val, sub_test = split_811(demo_data, np.unique(demo_data['lesion_id']))\n",
    "\n",
    "sub_train.to_csv('your_path/fariness_data/HAM10000/split/new_train.csv')\n",
    "sub_val.to_csv('your_path/fariness_data/HAM10000/split/new_val.csv')\n",
    "sub_test.to_csv('your_path/fariness_data/HAM10000/split/new_test.csv')\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['Path']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.dataframe.iloc[idx]['binaryLabel']\n",
    "        protected_attr = self.dataframe.iloc[idx]['Age_Category']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, protected_attr\n",
    "\n",
    "# Define data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomDataset(dataframe=sub_train, transform=transform)\n",
    "val_dataset = CustomDataset(dataframe=sub_val, transform=transform)\n",
    "test_dataset = CustomDataset(dataframe=sub_test, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the models\n",
    "model_resnet18 = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_resnet18.fc.in_features\n",
    "model_resnet18.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_vgg16 = models.vgg16(pretrained=True)\n",
    "num_ftrs = model_vgg16.classifier[6].in_features\n",
    "model_vgg16.classifier[6] = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels, _ in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        print()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the models\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_resnet18 = model_resnet18.to(device)\n",
    "model_vgg16 = model_vgg16.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_resnet18 = optim.SGD(model_resnet18.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_vgg16 = optim.SGD(model_vgg16.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'val': val_loader\n",
    "}\n",
    "\n",
    "model_names = ['ResNet18', 'VGG16']\n",
    "trained_models = []\n",
    "\n",
    "for model, optimizer, model_name in zip([model_resnet18, model_vgg16], [optimizer_resnet18, optimizer_vgg16], model_names):\n",
    "    print(f\"Training {model_name}...\")\n",
    "    trained_model = train_model(model, dataloaders, criterion, optimizer, num_epochs=25)\n",
    "    trained_models.append(trained_model)\n",
    "    torch.save(trained_model.state_dict(), f'model_{model_name}.pth')\n",
    "\n",
    "# Function to calculate demographic parity\n",
    "def demographic_parity(labels, preds, protected_attrs):\n",
    "    protected_groups = np.unique(protected_attrs)\n",
    "    parity = {}\n",
    "    for group in protected_groups:\n",
    "        group_mask = (protected_attrs == group)\n",
    "        parity[group] = np.mean(preds[group_mask] == 1)\n",
    "    return parity\n",
    "\n",
    "# Function to calculate equalized odds\n",
    "def equalized_odds(labels, preds, protected_attrs):\n",
    "    protected_groups = np.unique(protected_attrs)\n",
    "    tpr = {}\n",
    "    fpr = {}\n",
    "    for group in protected_groups:\n",
    "        group_mask = (protected_attrs == group)\n",
    "        tpr[group] = np.mean(preds[group_mask][labels[group_mask] == 1] == 1)\n",
    "        fpr[group] = np.mean(preds[group_mask][labels[group_mask] == 0] == 1)\n",
    "    return tpr, fpr\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(labels, preds):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    return accuracy, f1\n",
    "\n",
    "# Function to evaluate bias and performance in the model\n",
    "def evaluate_model(model, dataloader, protected_attr_name):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_protected_attrs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, protected_attrs in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_protected_attrs.extend(protected_attrs)\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_protected_attrs = np.array(all_protected_attrs)\n",
    "\n",
    "    dp = demographic_parity(all_labels, all_preds, all_protected_attrs)\n",
    "    eo = equalized_odds(all_labels, all_preds, all_protected_attrs)\n",
    "    accuracy, f1 = calculate_metrics(all_labels, all_preds)\n",
    "\n",
    "    return dp, eo, accuracy, f1\n",
    "\n",
    "# Evaluate fairness and performance metrics for each trained model\n",
    "for model, model_name in zip(trained_models, model_names):\n",
    "    print(f\"Evaluating fairness and performance metrics for {model_name}...\")\n",
    "    dp, eo, accuracy, f1 = evaluate_model(model, test_loader, protected_attr_name='Age_Category')\n",
    "    print(f\"Demographic Parity for {model_name}: {dp}\")\n",
    "    print(f\"Equalized Odds for {model_name}: {eo}\")\n",
    "    print(f\"Accuracy for {model_name}: {accuracy}\")\n",
    "    print(f\"F1 Score for {model_name}: {f1}\")\n",
    "    print()\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
